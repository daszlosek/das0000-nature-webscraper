---
title: "Web Scraper"
output: html_notebook
params:
  nb_id: "001"
---


```{r load packages}

packages = c('rmarkdown', 'plyr', 'dplyr','ggplot2', 'readr', 'tidyr', 'stringr',  'knitr', 'sparklyr', 'shiny', 'data.table', 'zoo','fasttime',"twitteR","openssl","httpuv" , 'rvest', 'httr','jsonlite','lubridate', 'twitteR')

package.check <- lapply(packages, FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
        install.packages(x, dependencies = TRUE)
        library(x, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)
    }
})


```




```{r testing rvest}

# pull in some html

nsmb <- read_html("https://www.nature.com/nsmb/volumes/25/issues/10") 

# https://www.nature.com/nsmb/volumes/25/issues/10

```




```{r}


nsmb %>% rvest::html_text()

```






```{r}

nsmb %>% html_node(paste0(".pb30:nth-child(1) a")) %>% html_text()


nsmb %>% html_node(paste0(".pb30:nth-child(2) a")) %>% html_text()



nsmb %>% html_node(paste0(".pb30:nth-child(7) a")) %>% html_text()



```






```{r}

# testing cleaning up text
nsmb %>% html_node(paste0(".pb30:nth-child(7) a")) %>% html_text2()


```





```{r }
#function to pull out subjournal, volume and issue information

subjournal <- "nsmb"

subjournal_html_parser <- function(subjournal = "nsmb",volume = 1,issue = 1,child_node = 1) {
          tmp_html <- read_html(paste0("https://www.nature.com/",subjournal,"/volumes/",volume,"/issues/",issue))
          
           
          # parse html text
          parsed_title <- tmp_html %>% html_node(paste0(".pb30:nth-child(",child_node,") a")) %>% html_text2()
          
          bind_cols(subjournal = subjournal,
                    volume = volume,
                    issue = issue,
                    parsed_title = parsed_title,
                    child_node = child_node) 
}


subjournal_html_parser(subjournal = "nsmb",volume = 1,issue = 12,child_node = 7)

subjournal_html_parser(subjournal = "nsmb",volume = 1,issue = 13,child_node = 7)
```




```{r }
# skip errors 
subjournal_html_parser2 <- possibly(subjournal_html_parser, otherwise = NA)

subjournal_html_parser2(subjournal = "nsmb",volume = 1,issue = 13,child_node = 7)



```




```{r}

# messing around with lapply 


tictoc::tic()
test_list <- lapply(1:2, subjournal_html_parser, subjournal = "nsmb", issue = c(1,2), volume = 1)
tictoc::toc()

bind_rows(test_list)


```



```{r}
#messing around with mapply

tictoc::tic()
test  <- mapply(subjournal_html_parser2, issue = 1, child_node = 1:12, subjournal = "nsmb",volume = 1) 
tictoc::toc()
test2 <- test %>% t() #%>% filter(!is.na(parsed_title))

test3 <- bind_rows(test2)

#  according to the help mapply "applies FUN to the first elements of each ... argument, the second elements, the third elements, and so on.". And what I want is to apply FUN to all the different combinations of all the arguments.
```





```{r}

# expand grid to give all combinations 
cmapply <- function(FUN, ..., MoreArgs = NULL, SIMPLIFY = TRUE, USE.NAMES = TRUE) {
    # expand grid to prepare the combinations
    all_comb <- expand.grid(..., stringsAsFactors=FALSE)
    # execute mapply on all combinations 
    vector_output <- do.call(mapply, c(
        
        list(FUN=FUN, MoreArgs = MoreArgs, SIMPLIFY = SIMPLIFY, USE.NAMES = USE.NAMES),all_comb))
    # turn vectpr output into list of matrices 
    if (is.matrix(vector_output)) vector_output <- t(vector_output) 
    # turn list of matrices into df
    cbind(all_comb, vector_output)
}


```


```{r pull article data}

# get article
# it is not as  efficient as I would like but then again I am webscrapping almost 30 years of article titles
article_combination_list <- cmapply(issue = 1:2, child_node = 1:2, subjournal = "nsmb",volume = 1:2, FUN=subjournal_html_parser2) 



article_data_raw <- bind_rows(article_combination_list) %>% janitor::clean_names()



```











































