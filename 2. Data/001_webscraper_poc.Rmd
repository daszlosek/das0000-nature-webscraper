---
title: "Web Scraper POC"
output: html_notebook
params:
  nb_id: "001"
---


```{r load packages}

packages = c('rmarkdown', 'plyr', 'dplyr','ggplot2', 'readr', 'tidyr', 'stringr',  'knitr', 'sparklyr', 'shiny', 'data.table', 'zoo','fasttime',"twitteR","openssl","httpuv" , 'rvest', 'httr','jsonlite','lubridate', 'twitteR','magrittr')

package.check <- lapply(packages, FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
        install.packages(x, dependencies = TRUE)
        library(x, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)
    }
})
library("rvest")

```

```{r}

read_html("https://www.nature.com/nsmb/volumes/25/issues/10") 

```



```{r testing rvest}

# pull in some html

nsmb <- read_html("https://www.nature.com/nsmb/volumes/25/issues/10") 


nsmb
# https://www.nature.com/nsmb/volumes/25/issues/10


```



```{r}

read_html("https://www.nature.com/nsmb/volumes/25/issues/10") %>% 
          html_node(paste0(".pb30:nth-child(1) a")) %>% html_text()


nsmb %>% html_node(paste0(".pb30:nth-child(2) a")) %>% html_text()



nsmb %>% html_node(paste0(".pb30:nth-child(7) a")) %>% html_text()


nsmb %>% html_node(paste0("time")) %>% html_text()


```


```{r}

# testing cleaning up text
nsmb %>% html_node(paste0(".pb30:nth-child(1) a")) %>% html_text(.,trim = T)


```


```{r }
#function to pull out sub journal, volume and issue information

subjournal_html_parser <- function(subjournal = "nsmb",volume = 1,issue = 1,child_node = 1) {
          tmp_html <- read_html(paste0("https://www.nature.com/",subjournal,"/volumes/",volume,"/issues/",issue))
          
          # parse html text
          parsed_title <- tmp_html %>% html_node(paste0(".pb30:nth-child(",child_node,") a")) %>% html_text(.,trim = T)
          # tacked on as an afterthought...whoops
          date <- tmp_html %>% html_node(paste0("time")) %>% html_text(.,trim = T)
          
          bind_cols(
                    date = date,
                    parsed_title = parsed_title) 
}
```


```{r }
subjournal_html_parser(subjournal = "nsmb",volume = 1,issue = 12,child_node = 7)

subjournal_html_parser(subjournal = "nsmb",volume = 1,issue = 13,child_node = 7)
```




```{r }
# skip errors 
nature_html_parser <- purrr::possibly(subjournal_html_parser, otherwise = NA)

nature_html_parser(subjournal = "nsmb",volume = 1,issue = 13,child_node = 7)

nature_html_parser(subjournal = "nsmb",volume = 1,issue = 12,child_node = 7)

# save out nature parser function
write_rds(nature_html_parser, here::here("2. Data",paste0(params$nb_id,"_nature_html_parser.rds")))

```


```{r}

# expand grid to give all combinations 
cmapply_raw <- function(FUN, ..., MoreArgs = NULL, SIMPLIFY = TRUE, USE.NAMES = TRUE) {
    # expand grid to prepare the combinations
    all_comb <- expand.grid(..., stringsAsFactors=FALSE)
    # execute mapply on all combinations 
    vector_output <- do.call(mapply, c(
        
        list(FUN=FUN, MoreArgs = MoreArgs, SIMPLIFY = SIMPLIFY, USE.NAMES = USE.NAMES),all_comb))
    # transpose matrix
    if (is.matrix(vector_output)) vector_output <- t(vector_output) 

# vector length divided by two due to title and date, to give two columns one for date and one for title
vct_length <- (length(vector_output)/2)

# all this work just to add a time stamp.....
parse_title <- vector_output %>% data.frame() %>% unlist %>% data.frame() %>%
              tibble::rownames_to_column(., "name") %>%
              dplyr::rename("value" = 2) %>%
              mutate(name = ifelse(str_detect(name,"date"),"date","parsed_title")) %>%
              group_by(name) %>%
              mutate(id = row_number()) %>% ungroup() %>%
              pivot_wider(., names_from = name, values_from = value)

cbind(all_comb, parse_title)
}



cmapply <- purrr::possibly(cmapply_raw, otherwise = NA)

# # save out cmapply function
write_rds(cmapply, here::here("2. Data",paste0(params$nb_id,"_cmapply.rds")))


```


```{r pull article data}

# get article
# it is not as  efficient as I would like but then again I am webscrapping almost 30 years of article titles
article_combination_list <- cmapply(issue = 1:3, child_node = 1:2, subjournal = c("nsmb","leu"),volume = 12:13, FUN=nature_html_parser)  

article_combination_list
```

```{r rvest journal code extraction}

# pull in some html from the journal index page
site_index_raw <- read_html("https://www.nature.com/siteindex") 

site_index_raw %>% html_nodes("a") %>% html_attr("href")

# clean it up and remove excess
site_index_clean <- site_index_raw %>% html_nodes("a") %>% html_attr("href") %>% data.frame() %>%
               filter(str_detect(.,"#|https|.com|myaccount|search|search|authors|java|naturecareers|subject") != T ) %>%
               # clean up journal codes
               transmute(journal_code = str_replace_all(.,"/","")) %>%
               # turn into vector
               filter(journal_code != "")

site_index_clean
# other journals of interest
other_journal_codes <- c("nchembio", "nclimate", "natcomputsci", "nenergy", "ng", "nathumbehav", "nmat", "natmetab", "nmicrobiol", "neuro", "nphys", "nprot", "natsustain", "natastron", "nbt", "natcardiovascres", "ncb", "nchem", "natelectron", "ngeo", "ni", "natmachintell", "nm", "nmeth", "nnano", "nphoton", "nplants", "nsmb", "natsynth")

# vector
journal_code_vector <- site_index_clean %>% 
           filter(str_detect(journal_code,"nat"),
                  str_detect(journal_code,"italy|africa|rev|nature") != T) %$%
           journal_code %>% c(.,other_journal_codes) %>% unique()
           

# save out the journal code vector 
readr::write_rds(journal_code_vector, here::here("2. Data", paste0(params$nb_id,"_journal_code_vector.rds")))

```































