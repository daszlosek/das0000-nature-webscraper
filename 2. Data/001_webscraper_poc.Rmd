---
title: "Web Scraper POC"
output: html_notebook
params:
  nb_id: "001"
---


```{r load packages}

packages = c('rmarkdown', 'plyr', 'dplyr','ggplot2', 'readr', 'tidyr', 'stringr',  'knitr', 'sparklyr', 'shiny', 'data.table', 'zoo','fasttime',"twitteR","openssl","httpuv" , 'rvest', 'httr','jsonlite','lubridate', 'twitteR','magrittr')

package.check <- lapply(packages, FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
        install.packages(x, dependencies = TRUE)
        library(x, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)
    }
})


```


```{r testing rvest}

# pull in some html

nsmb <- read_html("https://www.nature.com/nsmb/volumes/25/issues/10") 

# https://www.nature.com/nsmb/volumes/25/issues/10

```


```{r}


nsmb %>% rvest::html_text()

```


```{r}

nsmb %>% html_node(paste0(".pb30:nth-child(1) a")) %>% html_text()


nsmb %>% html_node(paste0(".pb30:nth-child(2) a")) %>% html_text()



nsmb %>% html_node(paste0(".pb30:nth-child(7) a")) %>% html_text()



```


```{r}

# testing cleaning up text
nsmb %>% html_node(paste0(".pb30:nth-child(7) a")) %>% html_text2()


```


```{r }
#function to pull out subjournal, volume and issue information

subjournal <- "nsmb"

subjournal_html_parser <- function(subjournal = "nsmb",volume = 1,issue = 1,child_node = 1) {
          tmp_html <- read_html(paste0("https://www.nature.com/",subjournal,"/volumes/",volume,"/issues/",issue))
          
           
          # parse html text
          parsed_title <- tmp_html %>% html_node(paste0(".pb30:nth-child(",child_node,") a")) %>% html_text2()
          
          bind_cols(subjournal = subjournal,
                    volume = volume,
                    issue = issue,
                    parsed_title = parsed_title,
                    child_node = child_node) 
}


subjournal_html_parser(subjournal = "nsmb",volume = 1,issue = 12,child_node = 7)

subjournal_html_parser(subjournal = "nsmb",volume = 1,issue = 13,child_node = 7)
```




```{r }
# skip errors 
nature_html_parser <- purrr::possibly(subjournal_html_parser, otherwise = NA)

nature_html_parser(subjournal = "nsmb",volume = 1,issue = 13,child_node = 7)


# save out nature parser function
write_rds(nature_html_parser, here::here("2. Data",paste0(params$nb_id,"nature_html_parser.rds")))

```




```{r}

# messing around with lapply 


tictoc::tic()
test_list <- lapply(1:2, subjournal_html_parser, subjournal = "nsmb", issue = c(1,2), volume = 1)
tictoc::toc()

bind_rows(test_list)


```



```{r}
#messing around with mapply

tictoc::tic()
test  <- mapply(nature_html_parser, issue = 1, child_node = 1:12, subjournal = "nsmb",volume = 1) 
tictoc::toc()
test2 <- test %>% t() #%>% filter(!is.na(parsed_title))

test3 <- bind_rows(test2)

#  according to the help mapply "applies FUN to the first elements of each ... argument, the second elements, the third elements, and so on.". And what I want is to apply FUN to all the different combinations of all the arguments.
```





```{r}

# expand grid to give all combinations 
cmapply <- function(FUN, ..., MoreArgs = NULL, SIMPLIFY = TRUE, USE.NAMES = TRUE) {
    # expand grid to prepare the combinations
    all_comb <- expand.grid(..., stringsAsFactors=FALSE)
    # execute mapply on all combinations 
    vector_output <- do.call(mapply, c(
        
        list(FUN=FUN, MoreArgs = MoreArgs, SIMPLIFY = SIMPLIFY, USE.NAMES = USE.NAMES),all_comb))
    # turn vectpr output into list of matrices 
    if (is.matrix(vector_output)) vector_output <- t(vector_output) 
    # turn list of matrices into df
    cbind(all_comb, vector_output)
}



# save out cmapply function
write_rds(cmapply, here::here("2. Data",paste0(params$nb_id,"cmapply.rds")))


```


```{r pull article data}

# get article
# it is not as  efficient as I would like but then again I am webscrapping almost 30 years of article titles
article_combination_list <- cmapply(issue = 1:2, child_node = 1:2, subjournal = "nsmb",volume = 1:2, FUN=nature_html_parser) 



article_data_raw <- bind_rows(article_combination_list) %>% janitor::clean_names()



```





```{r}


#list out some of the articles
journals <- c("nsmb","leu","natcancer","natcardiovascres","nmeth")



```


```{r rvest journal code extraction}

# pull in some html from the journal index page
site_index_raw <- read_html("https://www.nature.com/siteindex") 

site_index_raw %>% html_nodes("a") %>% html_attr("href")

# clean it up and remove excess
site_index_clean <- site_index_raw %>% html_nodes("a") %>% html_attr("href") %>% data.frame() %>%
               filter(str_detect(.,"#|https|.com|myaccount|search|search|authors|java|naturecareers|subject") != T ) %>%
               # clean up journal codes
               transmute(journal_code = str_replace_all(.,"/","")) %>%
               # turn into vector
               filter(journal_code != "")

# vectorize
journal_code_vector <- site_index_clean3 %$% journal_code
               


# save out the journal code vector 
readr::write_rds(journal_code_vector, here::here("2. Data", paste0(params$nb_id,"_journal_code_vector.rds")))



```































