---
title: "Web Scraper Data Pull"
output: html_notebook
params:
  nb_id: "002"
---


```{r load packages}

packages = c('rmarkdown', 'plyr', 'dplyr','ggplot2', 'readr', 'tidyr', 'stringr',  'knitr', 'sparklyr', 'shiny', 'data.table', 'zoo','fasttime',"twitteR","openssl","httpuv" , 'rvest', 'httr','jsonlite','lubridate', 'twitteR','magrittr')

package.check <- lapply(packages, FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
        install.packages(x, dependencies = TRUE)
        library(x, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)
    }
})


```


```{r load poc scripts}


nature_html_parser <- read_rds(here::here("2. Data",paste0("001","nature_html_parser.rds")))


cmapply <- readr::read_rds(here::here("2. Data", paste0("001","cmapply.rds")))


journal_code_vector <- readr::read_rds(here::here("2. Data", paste0("001","_journal_code_vector.rds")))


journal_code_vector

```






```{r pull article data}
# get article
# it is not as  efficient as I would like but then again I am webscrapping almost 30 years of article titles
article_combination_list <- cmapply(issue = 1:50, child_node = 1:15, subjournal = c("nsmb","leu"),volume = 1:50, FUN=nature_html_parser) 


article_combination_list
```


```{r pull article data}


raw_leu <- article_combination_list %>% filter(subjournal == "leu") %>% filter(!is.na(parse_title))


raw_leu %>% group_by(is.na(parse_title)) %>% count()

```