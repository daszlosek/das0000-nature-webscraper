---
title: "Web Scraper"
output: html_notebook
---



```{r}
#install.packages("rvest")

library(rvest)


rm(list=ls())

packages = c('rmarkdown', 'plyr', 'dplyr','ggplot2', 'readr', 'tidyr', 'stringr',  'knitr', 'sparklyr', 'shiny', 'data.table', 'zoo','fasttime',"twitteR","openssl","httpuv" , 'rvest', 'httr','jsonlite','lubridate')

package.check <- lapply(packages, FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
        install.packages(x, dependencies = TRUE)
        library(x, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)
    }
})


```

```{r}

nsmb <- read_html("https://www.nature.com/nsmb/volumes/25/issues/10") 

```

```{r artcile title}

loop_list <- 1:8

nsmb_test <- as.character()
nsmb_test_link <- as.character()


for (loop in loop_list){
  #article title
nsmb_node <-  nsmb %>% html_node(paste0(".pb30:nth-child(",loop,") a")) %>% html_text()  %>% 
        # Trim additional white space
        str_trim() %>%                       
        # Convert the list into a vector
        unlist()

nsmb_t0 <- str_trim(sapply(strsplit(nsmb_node, "\n"), "[", 1)) 

nsmb_test <- rbind(nsmb_test, nsmb_t0)


# article link
nsmb_node_link <-  nsmb %>% html_node(paste0(".pb30:nth-child(",loop,") a")) %>% html_attr('href')  %>% 
        # Trim additional white space
        str_trim() %>%                       
        # Convert the list into a vector
        unlist()

nsmb_t0_link <- str_trim(nsmb_node_link) 

nsmb_test_link <- rbind(nsmb_test_link, nsmb_t0_link)



}

nsmb2_link <- nsmb_test_link %>% data.frame() %>% transmute(link = (paste0("https://www.nature.com",.)))
row.names(nsmb2_link) <- NULL

nsmb2 <- nsmb_test %>% data.frame() %>% dplyr::rename(.,"title" = ".")
row.names(nsmb2) <- NULL

nsmb3 <- cbind(nsmb_test,nsmb2_link)

rm(loop, loop_list, nsmb_test,nsmb_t0, nsmb_node, nsmb_node_link, nsmb_t0_link, nsmb_test_link, nsmb2_link, nsmb2)

```














































```{r}

url_nat <- 'http://api.nature.com'
# Construct API request
repos <- GET(url = url_nat)

names(repos)
#check to see if api is working or if i fucked up
status_code(repos) # 200..looking good


repo_content <- content(repos)

head(repos$headers,2)

repo_df <- lapply(repo_content, function(x) {
  df <- data_frame(repo        = x$name,
                   address     = x$html_url,
                   commits     = x$git_commits_url)
}) %>% bind_rows()
```


```{r API tutorial}

# Save username as variable
username <- 'hadley'

# Save base enpoint as variable
url_git <- 'https://api.github.com/'

# Construct API request
repos <- GET(url = paste0(url_git,'users/',username,'/repos'))


# Examine response components
names(repos)

status_code(repos)

# Process API request content 
repo_content <- content(repos)

# Apply function across all list elements to extract the name and address of each repo
repo_df <- lapply(repo_content, function(x) {
  df <- data_frame(repo        = x$name,
                   address     = x$html_url,
                   commits     = x$git_commits_url)
}) %>% bind_rows()

repos
```


